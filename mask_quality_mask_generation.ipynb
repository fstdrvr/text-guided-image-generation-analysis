{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "uKs2L0tg8VdI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCyKXpoK8Taj",
        "outputId": "4182dc88-7052-40c0-b4c4-201571a4967d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'clipseg'...\n",
            "remote: Enumerating objects: 226, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 226 (delta 66), reused 51 (delta 51), pack-reused 141\u001b[K\n",
            "Receiving objects: 100% (226/226), 1.40 MiB | 6.11 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/timojl/clipseg.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Commits on Sep 27, 2022\n",
        "%cd /content/clipseg\n",
        "!git checkout 515ca6ec2d066d447240c1dd79f3bbbee685bd29"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M98t-VsqMpQx",
        "outputId": "f5eaca27-84cc-4177-e492-fef1f65ab36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/clipseg\n",
            "Note: switching to '515ca6ec2d066d447240c1dd79f3bbbee685bd29'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 515ca6e added samples for fine-grained weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git@d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
        "\n",
        "%cd /content/clipseg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVlljphrMuKd",
        "outputId": "9ff9f59c-e882-463d-ab13-d38cea7df02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git@d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
            "  Cloning https://github.com/openai/CLIP.git (to revision d50d76daa670286dd6cacf3bcd80b5e4823fc8e1) to /tmp/pip-req-build-m36cimab\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-m36cimab\n",
            "  Running command git rev-parse -q --verify 'sha^d50d76daa670286dd6cacf3bcd80b5e4823fc8e1'\n",
            "  Running command git fetch -q https://github.com/openai/CLIP.git d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
            "  Running command git checkout -q d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
            "  Resolved https://github.com/openai/CLIP.git to commit d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.1+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369398 sha256=c158305ee46998a6b0c2f55d7fbaddc18cca19e62db587f84c885a462b16ed91\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/93/51/4bad85c7f917afc40abc2efdf783bfc4944cbb3f535c54da76\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "/content/clipseg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "from models.clipseg import CLIPDensePredT\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"using device is\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvv1ZbstM0UH",
        "outputId": "bd569494-4d74-4c22-dc16-dbe17e3d8a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pretrained\n",
        "\n",
        "if not os.path.exists('pretrained/weights.zip'):\n",
        "  !wget https://owncloud.gwdg.de/index.php/s/ioHbRzFx6th32hn/download -O pretrained/weights.zip\n",
        "  !unzip -d pretrained/weights -j pretrained/weights.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofKSBTdGM6Rb",
        "outputId": "3eec2592-970e-4af8-f5ad-6d1e8b650092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-07 09:34:17--  https://owncloud.gwdg.de/index.php/s/ioHbRzFx6th32hn/download\n",
            "Resolving owncloud.gwdg.de (owncloud.gwdg.de)... 134.76.23.45\n",
            "Connecting to owncloud.gwdg.de (owncloud.gwdg.de)|134.76.23.45|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘pretrained/weights.zip’\n",
            "\n",
            "pretrained/weights.     [              <=>   ]   9.68M  3.51MB/s    in 2.8s    \n",
            "\n",
            "2023-05-07 09:34:21 (3.51 MB/s) - ‘pretrained/weights.zip’ saved [10146905]\n",
            "\n",
            "Archive:  pretrained/weights.zip\n",
            " extracting: pretrained/weights/rd16-uni.pth  \n",
            " extracting: pretrained/weights/rd64-uni-refined.pth  \n",
            " extracting: pretrained/weights/rd64-uni.pth  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model = CLIPDensePredT(version='ViT-B/16', reduce_dim=64)\n",
        "model.eval();\n",
        "\n",
        "model.load_state_dict(torch.load('pretrained/weights/rd64-uni.pth', map_location=torch.device(device)), strict=False);"
      ],
      "metadata": {
        "id": "JgHhBou2M-Qj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99f6263-4931-46ff-be2a-ad8304c47cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 335M/335M [00:02<00:00, 160MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mask Manipulation function\n"
      ],
      "metadata": {
        "id": "9zpTWeMqAoeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy \n",
        "'''\n",
        "function to adjust the noise level for a given mask\n",
        "input_mask: the 512x512 uint mask matrix\n",
        "noise_level: a float value between 0 and 1, represents how many noise to add according to number of value 255 inside the given mask (masked pixels)\n",
        "mode: 0 or 1 or 2\n",
        "      0 - only change originally masked pixels\n",
        "      1 - only change originally not masked pixels (across the entire image)\n",
        "      2 - change both masked and un masksed pixels (across the entire image)\n",
        "'''\n",
        "def add_noise_to_mask(input_mask, noise_level, mode=0):\n",
        "  output_mask = deepcopy(input_mask)\n",
        "  num_masked = np.count_nonzero(input_mask == 255)\n",
        "  masked_indices = np.asarray(np.where(input_mask == 255)).T\n",
        "  background_indices = np.asarray(np.where(input_mask == 0)).T\n",
        "  num_noise = int(noise_level*num_masked)\n",
        "\n",
        "  if mode == 0:\n",
        "    change_indices =[masked_indices[x] for x in np.random.choice(range(len(masked_indices)), size=int(noise_level*num_masked), replace=False)]\n",
        "    for x in change_indices:\n",
        "      output_mask[x[0]][x[1]] = 0\n",
        "  elif mode == 1:\n",
        "    change_indices =[background_indices[x] for x in np.random.choice(range(len(background_indices)), size=int(noise_level*num_masked), replace=False)]\n",
        "    for x in change_indices:\n",
        "      output_mask[x[0]][x[1]] = 255\n",
        "  else:\n",
        "    change_prob = float(noise_level*num_masked)/(512.0*512.0)\n",
        "    change_mask = np.random.choice([0, 255], size=(512,512), p=[1-change_prob, change_prob])\n",
        "    output_mask = np.add(output_mask, change_mask)\n",
        "    output_mask[output_mask==510] = 0\n",
        "    \n",
        "  output_mask = np.uint8(output_mask)\n",
        "  return output_mask\n",
        "\n"
      ],
      "metadata": {
        "id": "GT41KuxFAmGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# image processing functions"
      ],
      "metadata": {
        "id": "U0NDX-bkDlP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def download_image(url):\n",
        "    response = requests.get(url)\n",
        "    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "def norm_photo(photo):\n",
        "  transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    transforms.Resize((512, 512)),])\n",
        "  photo = transform(photo).unsqueeze(0)\n",
        "  return photo"
      ],
      "metadata": {
        "id": "ZZJpiNqbDn4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# single image\n",
        "noise_levels = [0.0001, 0.001, 0.01, 0.1]\n",
        "total_experiments = len(noise_levels)*3\n",
        "curr_experiments = 1\n",
        "\n",
        "\n",
        "seg_prompts = 'white horse'\n",
        "input_image = PIL.Image.open('example_image_2.jpg').resize((512,512))\n",
        "img = norm_photo(input_image)\n",
        "\n",
        "# save normalized input\n",
        "filename = \"input.jpg\"\n",
        "plt.imsave(filename, input_image)\n",
        "del filename\n",
        "\n",
        "# generate mask\n",
        "with torch.no_grad():\n",
        "    preds = model(img.repeat(len(seg_prompts),1,1,1), seg_prompts)[0]\n",
        "filename = \"ori_mask.jpg\"\n",
        "plt.imsave(filename,torch.sigmoid(preds[0][0]))\n",
        "img2 = cv2.imread(filename)\n",
        "gray_image = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "del img2\n",
        "(thresh, bw_image) = cv2.threshold(gray_image, 100, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# mask array\n",
        "mask_arr = np.uint8(bw_image)\n",
        "\n",
        "# save original mask\n",
        "cv2.cvtColor(bw_image, cv2.COLOR_BGR2RGB)\n",
        "original_mask = Image.fromarray(np.uint8(bw_image)).convert('RGB').resize((512, 512))\n",
        "original_mask.save(filename)\n",
        "del filename\n",
        "\n",
        "# generate mask for all noise levels and modes\n",
        "for n,noise in enumerate(noise_levels):\n",
        "  for m in range(3):\n",
        "    # generate new mask\n",
        "    curr_mask = add_noise_to_mask(mask_arr, noise, mode=m)\n",
        "\n",
        "    # save new mask\n",
        "    curr_mask_img = Image.fromarray(curr_mask).convert('RGB')\n",
        "    filename = \"mask_mode\"+str(m)+\"_noise\"+str(n)+\".jpg\"\n",
        "    curr_mask_img.save(filename)\n",
        "\n",
        "    # print progress\n",
        "    print(\"current progress: %d/%d, %s\" % (curr_experiments, total_experiments, filename))\n",
        "    curr_experiments += 1\n",
        "\n",
        "    del curr_mask\n",
        "    del curr_mask_img\n",
        "    del filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ5o0QvCRt9K",
        "outputId": "6bfb6a74-1a90-4a2e-df91-b32c6b3fd9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current progress: 1/12, mask_mode0_noise0.jpg\n",
            "current progress: 2/12, mask_mode1_noise0.jpg\n",
            "current progress: 3/12, mask_mode2_noise0.jpg\n",
            "current progress: 4/12, mask_mode0_noise1.jpg\n",
            "current progress: 5/12, mask_mode1_noise1.jpg\n",
            "current progress: 6/12, mask_mode2_noise1.jpg\n",
            "current progress: 7/12, mask_mode0_noise2.jpg\n",
            "current progress: 8/12, mask_mode1_noise2.jpg\n",
            "current progress: 9/12, mask_mode2_noise2.jpg\n",
            "current progress: 10/12, mask_mode0_noise3.jpg\n",
            "current progress: 11/12, mask_mode1_noise3.jpg\n",
            "current progress: 12/12, mask_mode2_noise3.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "ZJ7cSYy1CZe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = ['http://farm6.staticflickr.com/5043/5306210272_5ae5fe553e_z.jpg',\n",
        "'http://farm6.staticflickr.com/5060/5514795117_aba57ba82f_z.jpg',\n",
        "'http://farm7.staticflickr.com/6228/6242985772_7253a1e774_z.jpg',\n",
        "'http://farm1.staticflickr.com/24/35275654_7d9397809b_z.jpg',\n",
        "'http://farm6.staticflickr.com/5107/5636416098_bfc492a357_z.jpg',\n",
        "'http://farm3.staticflickr.com/2823/9222897874_93f59af0a6_z.jpg',\n",
        "'http://farm3.staticflickr.com/2094/2435518227_10ab34b7c1_z.jpg',\n",
        "'http://farm7.staticflickr.com/6096/6287255884_22514e5d17_z.jpg',\n",
        "'http://farm7.staticflickr.com/6028/5949876155_74817efc59_z.jpg',\n",
        "'http://farm5.staticflickr.com/4045/4212548498_1f8c8110aa_z.jpg',\n",
        "'http://farm4.staticflickr.com/3454/3356237814_518e49c795_z.jpg',\n",
        "'http://farm6.staticflickr.com/5452/9205515621_4ce6319d66_z.jpg',\n",
        "'http://farm4.staticflickr.com/3504/3700248299_89346cf5ae_z.jpg',\n",
        "'http://farm8.staticflickr.com/7206/7094512025_391c37f4df_z.jpg',\n",
        "'http://farm9.staticflickr.com/8075/8324011371_68fca0267c_z.jpg',\n",
        "'http://farm1.staticflickr.com/38/116015759_cbabdbf4d6_z.jpg',\n",
        "'http://farm4.staticflickr.com/3118/2296947987_266e04ea63_z.jpg',\n",
        "'http://farm1.staticflickr.com/157/439127630_7deea02049_z.jpg',\n",
        "'http://farm5.staticflickr.com/4017/4515100161_f9956e4b58_z.jpg',\n",
        "'http://farm2.staticflickr.com/1289/766387238_331186506a_z.jpg',\n",
        "'http://farm3.staticflickr.com/2531/3797908806_a0381dcc3b_z.jpg',\n",
        "'http://farm9.staticflickr.com/8363/8335892848_8280029ef5_z.jpg',\n",
        "'http://farm1.staticflickr.com/104/281539229_c54f5c027a_z.jpg',\n",
        "'http://farm1.staticflickr.com/157/336331930_9be45527ac_z.jpg',\n",
        "'http://farm4.staticflickr.com/3034/2570055632_3d9e81ce35_z.jpg']\n",
        "\n",
        "prompts = [\"elephant\", \"clock\", \"husky\", \"laptop computer\", \"black and white bird\",\n",
        "           \"person\",\"yellow fire hydrant\", \"airplane\", \"small plane\",\"person\", \"suitcase\",\n",
        "           \"yellow rose\",\"woman\",\"orange and purple tulip\", \"white microwave\", \"microwave oven\", \"sandwich\", \"cat\",\n",
        "           \"strawberries\", \"hot dog\", \"backpack\", \"bed\", \"suitcase\", \"wii remote\", \"couch\"]\n",
        "\n",
        "re_prompts = [\"tiger\", \"window\", \"cat\", \"stack of books\", \"squirrel\",\n",
        "           \"black and white mickey mouse\",\"metal dustbin\", \"fighter jet\", \"house\",\"dog\", \"gray computer\",\n",
        "           \"pink tulip\",\"monkey\",\"apple\", \"white suitcase\", \"tv\", \"omelette\", \"cabybara\",\n",
        "           \"oranges\", \"cookies\", \"dog\", \"pool\", \"stack of books\", \"magic wand\", \"pile of rocks\"]\n",
        "\n",
        "'''\n",
        "full_prompts = [\"an tiger walking through a forested area\",\n",
        "              \"a window that is next to an eagle statue\",\n",
        "              \"a large cat sitting and waiting for a treat\",\n",
        "              \"a stack of books sitting on top of a table\",\n",
        "              \"a squirrel sitting in an evergreen tree\",\n",
        "              \"a black and white mickey mouse jumping a skate board in the air\",\n",
        "              \"a metal dustbin on the side of a street\",\n",
        "              \"an fighter jet parked in a spot at the airport\",\n",
        "              \"A house that is sitting on a runway\",\n",
        "              \"a dog entering a crosswalk along a city street\",\n",
        "              \"a gray computer sitting on the floor \",\n",
        "              \"a vase sitting in the sun holding a pink tulip\",\n",
        "              \"a monkey opening the doors from a bathroom\",\n",
        "              \"a vase of apple on a wood table\",\n",
        "              \"a kitchen with a white suitacase oven sitting on top of a counter\",\n",
        "              \"a tv on top of a stove\",\n",
        "              \"a bowl of soup sitting on a plate with omelette and spoon near a basket of crackers\",\n",
        "              \"a gray cabybara lying on a carpet in a room\",\n",
        "              \"two bagels next to three oranges on a white plate\",\n",
        "              \"a couple of cookies sitting on a blue plate on a computer desk\",\n",
        "              \"a dog sits on a small bed in a small room\",\n",
        "              \"a pool is on the floor on wooden slats\",\n",
        "              \"there is a stack of books laid out in front of a couch\",\n",
        "              \"a girl standing up with an arm outstretched and a magic wand in hand\",\n",
        "              \"a laptop sitting on a pile of rocks next to a remote control\"]\n",
        "'''\n",
        "\n",
        "noise_levels = [0.001, 0.1]"
      ],
      "metadata": {
        "id": "AQTcRgxWA2Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Segmentation and Export mask photoes"
      ],
      "metadata": {
        "id": "rc3b9TplC4ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_experiments = len(noise_levels)*len(urls)*3\n",
        "curr_experiments = 1\n",
        "\n",
        "for url_i,url in enumerate(urls):\n",
        "  seg_prompts = prompts[url_i]\n",
        "  input_image = download_image(url).resize((512, 512))\n",
        "  img = norm_photo(input_image)\n",
        "\n",
        "  # save normalized input\n",
        "  filename = \"input_\"+str(url_i)+\".jpg\"\n",
        "  plt.imsave(filename, img)\n",
        "  del filename\n",
        "\n",
        "  # generate mask\n",
        "  with torch.no_grad():\n",
        "      preds = model(img.repeat(len(seg_prompts),1,1,1), seg_prompts)[0]\n",
        "  filename = \"input_mask_\"+str(url_i)+\".jpg\"\n",
        "  plt.imsave(filename,torch.sigmoid(preds[0][0]))\n",
        "  img2 = cv2.imread(filename)\n",
        "  gray_image = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "  del img2\n",
        "  (thresh, bw_image) = cv2.threshold(gray_image, 100, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "  # mask array\n",
        "  mask_arr = np.uint8(bw_image)\n",
        "\n",
        "  # save original mask\n",
        "  cv2.cvtColor(bw_image, cv2.COLOR_BGR2RGB)\n",
        "  original_mask = Image.fromarray(np.uint8(bw_image)).convert('RGB').resize((512, 512))\n",
        "  original_mask.save(filename)\n",
        "  del filename\n",
        "\n",
        "  # generate mask for all noise levels and modes\n",
        "  for n,noise in enumerate(noise_levels):\n",
        "    for m in range(3):\n",
        "      # generate new mask\n",
        "      curr_mask = add_noise_to_mask(mask_arr, noise, mode=m)\n",
        "\n",
        "      # save new mask\n",
        "      curr_mask_img = Image.fromarray(curr_mask).convert('RGB')\n",
        "      filename = \"edit_mask_\"+str(url_i)+\"_mode\"+str(m)+\"_noise\"+str(n)+\".jpg\"\n",
        "      curr_mask_img.save(filename)\n",
        "\n",
        "      # print progress\n",
        "      print(\"current progress: %d/%d, %s\" % (curr_experiments, total_experiments, filename))\n",
        "      curr_experiments += 1\n",
        "\n",
        "      del curr_mask\n",
        "      del curr_mask_img\n",
        "      del filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wrBZM6kCFPP",
        "outputId": "c54bd1aa-543f-4b6c-fa53-0e918d8606b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current progress: 1/150, edit_mask_0_mode0_noise0.jpg\n",
            "current progress: 2/150, edit_mask_0_mode1_noise0.jpg\n",
            "current progress: 3/150, edit_mask_0_mode2_noise0.jpg\n",
            "current progress: 4/150, edit_mask_0_mode0_noise1.jpg\n",
            "current progress: 5/150, edit_mask_0_mode1_noise1.jpg\n",
            "current progress: 6/150, edit_mask_0_mode2_noise1.jpg\n",
            "current progress: 7/150, edit_mask_1_mode0_noise0.jpg\n",
            "current progress: 8/150, edit_mask_1_mode1_noise0.jpg\n",
            "current progress: 9/150, edit_mask_1_mode2_noise0.jpg\n",
            "current progress: 10/150, edit_mask_1_mode0_noise1.jpg\n",
            "current progress: 11/150, edit_mask_1_mode1_noise1.jpg\n",
            "current progress: 12/150, edit_mask_1_mode2_noise1.jpg\n",
            "current progress: 13/150, edit_mask_2_mode0_noise0.jpg\n",
            "current progress: 14/150, edit_mask_2_mode1_noise0.jpg\n",
            "current progress: 15/150, edit_mask_2_mode2_noise0.jpg\n",
            "current progress: 16/150, edit_mask_2_mode0_noise1.jpg\n",
            "current progress: 17/150, edit_mask_2_mode1_noise1.jpg\n",
            "current progress: 18/150, edit_mask_2_mode2_noise1.jpg\n",
            "current progress: 19/150, edit_mask_3_mode0_noise0.jpg\n",
            "current progress: 20/150, edit_mask_3_mode1_noise0.jpg\n",
            "current progress: 21/150, edit_mask_3_mode2_noise0.jpg\n",
            "current progress: 22/150, edit_mask_3_mode0_noise1.jpg\n",
            "current progress: 23/150, edit_mask_3_mode1_noise1.jpg\n",
            "current progress: 24/150, edit_mask_3_mode2_noise1.jpg\n",
            "current progress: 25/150, edit_mask_4_mode0_noise0.jpg\n",
            "current progress: 26/150, edit_mask_4_mode1_noise0.jpg\n",
            "current progress: 27/150, edit_mask_4_mode2_noise0.jpg\n",
            "current progress: 28/150, edit_mask_4_mode0_noise1.jpg\n",
            "current progress: 29/150, edit_mask_4_mode1_noise1.jpg\n",
            "current progress: 30/150, edit_mask_4_mode2_noise1.jpg\n",
            "current progress: 31/150, edit_mask_5_mode0_noise0.jpg\n",
            "current progress: 32/150, edit_mask_5_mode1_noise0.jpg\n",
            "current progress: 33/150, edit_mask_5_mode2_noise0.jpg\n",
            "current progress: 34/150, edit_mask_5_mode0_noise1.jpg\n",
            "current progress: 35/150, edit_mask_5_mode1_noise1.jpg\n",
            "current progress: 36/150, edit_mask_5_mode2_noise1.jpg\n",
            "current progress: 37/150, edit_mask_6_mode0_noise0.jpg\n",
            "current progress: 38/150, edit_mask_6_mode1_noise0.jpg\n",
            "current progress: 39/150, edit_mask_6_mode2_noise0.jpg\n",
            "current progress: 40/150, edit_mask_6_mode0_noise1.jpg\n",
            "current progress: 41/150, edit_mask_6_mode1_noise1.jpg\n",
            "current progress: 42/150, edit_mask_6_mode2_noise1.jpg\n",
            "current progress: 43/150, edit_mask_7_mode0_noise0.jpg\n",
            "current progress: 44/150, edit_mask_7_mode1_noise0.jpg\n",
            "current progress: 45/150, edit_mask_7_mode2_noise0.jpg\n",
            "current progress: 46/150, edit_mask_7_mode0_noise1.jpg\n",
            "current progress: 47/150, edit_mask_7_mode1_noise1.jpg\n",
            "current progress: 48/150, edit_mask_7_mode2_noise1.jpg\n",
            "current progress: 49/150, edit_mask_8_mode0_noise0.jpg\n",
            "current progress: 50/150, edit_mask_8_mode1_noise0.jpg\n",
            "current progress: 51/150, edit_mask_8_mode2_noise0.jpg\n",
            "current progress: 52/150, edit_mask_8_mode0_noise1.jpg\n",
            "current progress: 53/150, edit_mask_8_mode1_noise1.jpg\n",
            "current progress: 54/150, edit_mask_8_mode2_noise1.jpg\n",
            "current progress: 55/150, edit_mask_9_mode0_noise0.jpg\n",
            "current progress: 56/150, edit_mask_9_mode1_noise0.jpg\n",
            "current progress: 57/150, edit_mask_9_mode2_noise0.jpg\n",
            "current progress: 58/150, edit_mask_9_mode0_noise1.jpg\n",
            "current progress: 59/150, edit_mask_9_mode1_noise1.jpg\n",
            "current progress: 60/150, edit_mask_9_mode2_noise1.jpg\n",
            "current progress: 61/150, edit_mask_10_mode0_noise0.jpg\n",
            "current progress: 62/150, edit_mask_10_mode1_noise0.jpg\n",
            "current progress: 63/150, edit_mask_10_mode2_noise0.jpg\n",
            "current progress: 64/150, edit_mask_10_mode0_noise1.jpg\n",
            "current progress: 65/150, edit_mask_10_mode1_noise1.jpg\n",
            "current progress: 66/150, edit_mask_10_mode2_noise1.jpg\n",
            "current progress: 67/150, edit_mask_11_mode0_noise0.jpg\n",
            "current progress: 68/150, edit_mask_11_mode1_noise0.jpg\n",
            "current progress: 69/150, edit_mask_11_mode2_noise0.jpg\n",
            "current progress: 70/150, edit_mask_11_mode0_noise1.jpg\n",
            "current progress: 71/150, edit_mask_11_mode1_noise1.jpg\n",
            "current progress: 72/150, edit_mask_11_mode2_noise1.jpg\n",
            "current progress: 73/150, edit_mask_12_mode0_noise0.jpg\n",
            "current progress: 74/150, edit_mask_12_mode1_noise0.jpg\n",
            "current progress: 75/150, edit_mask_12_mode2_noise0.jpg\n",
            "current progress: 76/150, edit_mask_12_mode0_noise1.jpg\n",
            "current progress: 77/150, edit_mask_12_mode1_noise1.jpg\n",
            "current progress: 78/150, edit_mask_12_mode2_noise1.jpg\n",
            "current progress: 79/150, edit_mask_13_mode0_noise0.jpg\n",
            "current progress: 80/150, edit_mask_13_mode1_noise0.jpg\n",
            "current progress: 81/150, edit_mask_13_mode2_noise0.jpg\n",
            "current progress: 82/150, edit_mask_13_mode0_noise1.jpg\n",
            "current progress: 83/150, edit_mask_13_mode1_noise1.jpg\n",
            "current progress: 84/150, edit_mask_13_mode2_noise1.jpg\n",
            "current progress: 85/150, edit_mask_14_mode0_noise0.jpg\n",
            "current progress: 86/150, edit_mask_14_mode1_noise0.jpg\n",
            "current progress: 87/150, edit_mask_14_mode2_noise0.jpg\n",
            "current progress: 88/150, edit_mask_14_mode0_noise1.jpg\n",
            "current progress: 89/150, edit_mask_14_mode1_noise1.jpg\n",
            "current progress: 90/150, edit_mask_14_mode2_noise1.jpg\n",
            "current progress: 91/150, edit_mask_15_mode0_noise0.jpg\n",
            "current progress: 92/150, edit_mask_15_mode1_noise0.jpg\n",
            "current progress: 93/150, edit_mask_15_mode2_noise0.jpg\n",
            "current progress: 94/150, edit_mask_15_mode0_noise1.jpg\n",
            "current progress: 95/150, edit_mask_15_mode1_noise1.jpg\n",
            "current progress: 96/150, edit_mask_15_mode2_noise1.jpg\n",
            "current progress: 97/150, edit_mask_16_mode0_noise0.jpg\n",
            "current progress: 98/150, edit_mask_16_mode1_noise0.jpg\n",
            "current progress: 99/150, edit_mask_16_mode2_noise0.jpg\n",
            "current progress: 100/150, edit_mask_16_mode0_noise1.jpg\n",
            "current progress: 101/150, edit_mask_16_mode1_noise1.jpg\n",
            "current progress: 102/150, edit_mask_16_mode2_noise1.jpg\n",
            "current progress: 103/150, edit_mask_17_mode0_noise0.jpg\n",
            "current progress: 104/150, edit_mask_17_mode1_noise0.jpg\n",
            "current progress: 105/150, edit_mask_17_mode2_noise0.jpg\n",
            "current progress: 106/150, edit_mask_17_mode0_noise1.jpg\n",
            "current progress: 107/150, edit_mask_17_mode1_noise1.jpg\n",
            "current progress: 108/150, edit_mask_17_mode2_noise1.jpg\n",
            "current progress: 109/150, edit_mask_18_mode0_noise0.jpg\n",
            "current progress: 110/150, edit_mask_18_mode1_noise0.jpg\n",
            "current progress: 111/150, edit_mask_18_mode2_noise0.jpg\n",
            "current progress: 112/150, edit_mask_18_mode0_noise1.jpg\n",
            "current progress: 113/150, edit_mask_18_mode1_noise1.jpg\n",
            "current progress: 114/150, edit_mask_18_mode2_noise1.jpg\n",
            "current progress: 115/150, edit_mask_19_mode0_noise0.jpg\n",
            "current progress: 116/150, edit_mask_19_mode1_noise0.jpg\n",
            "current progress: 117/150, edit_mask_19_mode2_noise0.jpg\n",
            "current progress: 118/150, edit_mask_19_mode0_noise1.jpg\n",
            "current progress: 119/150, edit_mask_19_mode1_noise1.jpg\n",
            "current progress: 120/150, edit_mask_19_mode2_noise1.jpg\n",
            "current progress: 121/150, edit_mask_20_mode0_noise0.jpg\n",
            "current progress: 122/150, edit_mask_20_mode1_noise0.jpg\n",
            "current progress: 123/150, edit_mask_20_mode2_noise0.jpg\n",
            "current progress: 124/150, edit_mask_20_mode0_noise1.jpg\n",
            "current progress: 125/150, edit_mask_20_mode1_noise1.jpg\n",
            "current progress: 126/150, edit_mask_20_mode2_noise1.jpg\n",
            "current progress: 127/150, edit_mask_21_mode0_noise0.jpg\n",
            "current progress: 128/150, edit_mask_21_mode1_noise0.jpg\n",
            "current progress: 129/150, edit_mask_21_mode2_noise0.jpg\n",
            "current progress: 130/150, edit_mask_21_mode0_noise1.jpg\n",
            "current progress: 131/150, edit_mask_21_mode1_noise1.jpg\n",
            "current progress: 132/150, edit_mask_21_mode2_noise1.jpg\n",
            "current progress: 133/150, edit_mask_22_mode0_noise0.jpg\n",
            "current progress: 134/150, edit_mask_22_mode1_noise0.jpg\n",
            "current progress: 135/150, edit_mask_22_mode2_noise0.jpg\n",
            "current progress: 136/150, edit_mask_22_mode0_noise1.jpg\n",
            "current progress: 137/150, edit_mask_22_mode1_noise1.jpg\n",
            "current progress: 138/150, edit_mask_22_mode2_noise1.jpg\n",
            "current progress: 139/150, edit_mask_23_mode0_noise0.jpg\n",
            "current progress: 140/150, edit_mask_23_mode1_noise0.jpg\n",
            "current progress: 141/150, edit_mask_23_mode2_noise0.jpg\n",
            "current progress: 142/150, edit_mask_23_mode0_noise1.jpg\n",
            "current progress: 143/150, edit_mask_23_mode1_noise1.jpg\n",
            "current progress: 144/150, edit_mask_23_mode2_noise1.jpg\n",
            "current progress: 145/150, edit_mask_24_mode0_noise0.jpg\n",
            "current progress: 146/150, edit_mask_24_mode1_noise0.jpg\n",
            "current progress: 147/150, edit_mask_24_mode2_noise0.jpg\n",
            "current progress: 148/150, edit_mask_24_mode0_noise1.jpg\n",
            "current progress: 149/150, edit_mask_24_mode1_noise1.jpg\n",
            "current progress: 150/150, edit_mask_24_mode2_noise1.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mwvmrBX6dxGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nn4995RtLOJS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}